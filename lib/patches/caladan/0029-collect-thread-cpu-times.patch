From b041787bfb42e674f0759e573ed8af18e60d3991 Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Wed, 5 Jun 2024 18:06:13 -0400
Subject: [PATCH 29/35] collect thread cpu times

---
 inc/runtime/thread.h |  3 +++
 runtime/sched.c      | 55 +++++++++++++++++++++++++++++++++-----------
 2 files changed, 45 insertions(+), 13 deletions(-)

diff --git a/inc/runtime/thread.h b/inc/runtime/thread.h
index 3dd9d72..5e4d0cd 100644
--- a/inc/runtime/thread.h
+++ b/inc/runtime/thread.h
@@ -125,6 +125,7 @@ struct thread {
     uint16_t        last_cpu;
     uint16_t        cur_kthread;
     uint64_t        ready_tsc;
+    uint64_t        total_cycles;
     struct thread_tf    tf;
     struct list_node    link;
     struct list_node    interruptible_link;
@@ -134,6 +135,8 @@ struct thread {
 #endif
 };
 
+extern uint64_t thread_get_total_cycles(thread_t *th);
+
 /*
  * High-level routines, use this API most of the time.
  */
diff --git a/runtime/sched.c b/runtime/sched.c
index e2d28f5..99997aa 100644
--- a/runtime/sched.c
+++ b/runtime/sched.c
@@ -344,7 +344,7 @@ static __noinline bool do_watchdog(struct kthread *l)
 static __noinline void schedule(void)
 {
 	struct kthread *r = NULL, *l = myk();
-	uint64_t start_tsc;
+	uint64_t prog_cycles, start_tsc;
 	thread_t *th;
 	unsigned int start_idx;
 	unsigned int iters = 0;
@@ -359,19 +359,21 @@ static __noinline void schedule(void)
 	th = perthread_read_stable(__self);
 	assert(th == thread_self());
 
+	/* update entry stat counters */
+	STAT(RESCHEDULES)++;
+	start_tsc = rdtsc();
+	prog_cycles = start_tsc - perthread_read_stable(last_tsc);
+	STAT(PROGRAM_CYCLES) += prog_cycles;
+
 	/* unmark busy for the stack of the last uthread */
 	if (likely(th != NULL)) {
 		store_release(&th->thread_running, false);
-		th->cur_kthread = NCPU;
+		th->total_cycles += prog_cycles;
+		store_release(&th->cur_kthread, NCPU);
 		perthread_store(__self, NULL);
 		th = NULL;
 	}
 
-	/* update entry stat counters */
-	STAT(RESCHEDULES)++;
-	start_tsc = rdtsc();
-	STAT(PROGRAM_CYCLES) += start_tsc - perthread_read_stable(last_tsc);
-
 	/* increment the RCU generation number (even is in scheduler) */
 	store_release(&l->rcu_gen, l->rcu_gen + 1);
 	ACCESS_ONCE(l->q_ptrs->rcu_gen) = l->rcu_gen;
@@ -480,7 +482,7 @@ done:
 
 	update_oldest_tsc(l);
 
-	th->cur_kthread = l->kthread_idx;
+	store_release(&th->cur_kthread, l->kthread_idx);
 
 	spin_unlock(&l->lock);
 
@@ -508,7 +510,7 @@ static __always_inline void enter_schedule(thread_t *curth)
 {
 	struct kthread *k = myk();
 	thread_t *th;
-	uint64_t now_tsc;
+	uint64_t now_tsc, prog_cycles;
 
 	assert_preempt_disabled();
 
@@ -534,7 +536,9 @@ static __always_inline void enter_schedule(thread_t *curth)
 	}
 
 	/* fast path: switch directly to the next uthread */
-	STAT(PROGRAM_CYCLES) += now_tsc - perthread_get_stable(last_tsc);
+	prog_cycles = now_tsc - perthread_get_stable(last_tsc);
+	STAT(PROGRAM_CYCLES) += prog_cycles;
+	curth->total_cycles += prog_cycles;
 	perthread_get_stable(last_tsc) = now_tsc;
 
 	/* pop the next runnable thread from the queue */
@@ -546,7 +550,7 @@ static __always_inline void enter_schedule(thread_t *curth)
 		drain_overflow(k);
 
 	update_oldest_tsc(k);
-	curth->cur_kthread = NCPU;
+	store_release(&curth->cur_kthread, NCPU);
 	th->cur_kthread = k->kthread_idx;
 	spin_unlock(&k->lock);
 
@@ -766,17 +770,41 @@ void thread_finish_yield(void)
 	schedule();
 }
 
+// Hacky way to calculate total cycle time.
+uint64_t thread_get_total_cycles(thread_t *th) {
+
+	unsigned int k = load_acquire(&th->cur_kthread);
+	uint64_t k_run_start, cycles = ACCESS_ONCE(th->total_cycles);
+	// th is not running on any kthread or we just missed it starting to run.
+	// Either way the last total_cycles should be accurate.
+	if (k == NCPU)
+		return cycles;
+
+	bool correct_k = false;
+	spin_lock_np(&ks[k]->lock);
+	correct_k = th->cur_kthread == k;
+	cycles = ACCESS_ONCE(th->total_cycles);
+	k_run_start = ks[k]->q_ptrs->run_start_tsc;
+	spin_unlock_np(&ks[k]->lock);
+
+	if (correct_k)
+		return cycles + rdtsc() - k_run_start;
+	return cycles;
+}
+
 void thread_finish_cede(void)
 {
 	struct kthread *k = myk();
 	thread_t *myth = thread_self();
-	uint64_t tsc = rdtsc();
+	uint64_t prog_cycles, tsc = rdtsc();
 
 	/* update stats and scheduler state */
 	myth->thread_running = false;
 	myth->last_cpu = k->curr_cpu;
 	perthread_store(__self, NULL);
-	STAT(PROGRAM_CYCLES) += tsc - perthread_get_stable(last_tsc);
+	prog_cycles = tsc - perthread_get_stable(last_tsc);
+	STAT(PROGRAM_CYCLES) += prog_cycles;
+	myth->total_cycles += prog_cycles;
 
 	/* mark ceded thread ready at head of runqueue */
 	thread_ready_head(myth);
@@ -861,6 +889,7 @@ static __always_inline thread_t *__thread_create(void)
 	th->cur_kthread = NCPU;
 	// Can be used to detect newly created thread.
 	th->ready_tsc = 0;
+	th->total_cycles = 0;
 	th->xsave_area_in_use = false;
 	atomic8_write(&th->interrupt_state, 0);
 
-- 
2.43.0

