From c8d30adbdcc7d983e8d682c2b27f52f51fc035d9 Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Sun, 31 Dec 2023 04:21:49 +0000
Subject: [PATCH 33/33] uintr xsave area

---
 inc/runtime/thread.h |  5 ++++-
 runtime/defs.h       | 36 +++++++++++++++++++++++++++++++++---
 runtime/kthread.c    |  3 +++
 runtime/sched.c      |  4 +++-
 runtime/stack.c      |  4 ++--
 5 files changed, 45 insertions(+), 7 deletions(-)

diff --git a/inc/runtime/thread.h b/inc/runtime/thread.h
index a24c65e1..ca526ef7 100644
--- a/inc/runtime/thread.h
+++ b/inc/runtime/thread.h
@@ -113,9 +113,13 @@ struct thread {
     bool        main_thread:1;
     bool        has_fsbase:1;
     bool        thread_ready:1;
+    bool        link_armed:1;
     bool        junction_thread;
     bool        thread_running;
     bool        in_syscall;
+    /* modified by interrupt handler; should not be shared with other bitfields */
+    bool        xsave_area_in_use:1;
+    bool        xsave_area_active:1;
     atomic8_t        interrupt_state;
     struct thread_tf    *entry_regs;
     unsigned long    junction_tstate_buf[22];
@@ -126,7 +130,6 @@ struct thread {
     struct thread_tf    tf;
     struct list_node    link;
     struct list_node    interruptible_link;
-    bool        link_armed;
 #ifdef GC
     struct list_node    gc_link;
     unsigned int        onk;
diff --git a/runtime/defs.h b/runtime/defs.h
index 7ff1d840..8ce5296d 100644
--- a/runtime/defs.h
+++ b/runtime/defs.h
@@ -39,6 +39,9 @@
 #define RUNTIME_WATCHDOG_US		50
 #define RUNTIME_RX_BATCH_SIZE		32
 
+#define XSAVE_AREA_SIZE (24 * KB)
+#define XSAVE_AREA_PTR_SIZE (XSAVE_AREA_SIZE / sizeof(uintptr_t))
+
 /*
  * Thread support
  */
@@ -72,6 +75,21 @@ struct stack {
 DECLARE_PERTHREAD(struct tcache_perthread, stack_pt);
 DECLARE_PERTHREAD(void *, runtime_stack);
 
+static __always_inline void *stack_to_tcache_handle(struct stack *s)
+{
+	/*
+	 * use the bottom page of the stack (before the xsave area) for the tcache's
+	 * intrusive list. This way we don't fault in any more pages than we need.
+	 */
+	return (void *)((uintptr_t)(s + 1) - XSAVE_AREA_SIZE - PGSIZE_4KB);
+}
+
+static __always_inline struct stack *stack_from_tcache_handle(void *handle)
+{
+	uintptr_t addr = (uintptr_t)handle + PGSIZE_4KB + XSAVE_AREA_SIZE;
+	return (struct stack *)addr - 1;
+}
+
 /**
  * stack_alloc - allocates a stack
  *
@@ -84,7 +102,7 @@ static inline struct stack *stack_alloc(void)
 	void *p = tcache_alloc(perthread_ptr(stack_pt));
 	if (unlikely(!p))
 		return NULL;
-	return container_of((uintptr_t (*)[STACK_PTR_SIZE])p, struct stack, usable);
+	return stack_from_tcache_handle(p);
 }
 
 /**
@@ -93,7 +111,7 @@ static inline struct stack *stack_alloc(void)
  */
 static inline void stack_free(struct stack *s)
 {
-	tcache_free(perthread_ptr(stack_pt), (void *)s->usable);
+	tcache_free(perthread_ptr(stack_pt), stack_to_tcache_handle(s));
 }
 
 #define RSP_ALIGNMENT	16
@@ -121,6 +139,17 @@ static inline uint64_t stack_init_to_rsp(struct stack *s, void (*exit_fn)(void))
 {
 	uint64_t rsp;
 
+	s->usable[STACK_PTR_SIZE - XSAVE_AREA_PTR_SIZE - 1] = (uintptr_t)exit_fn;
+	rsp = (uint64_t)&s->usable[STACK_PTR_SIZE - XSAVE_AREA_PTR_SIZE - 1];
+	assert_rsp_aligned(rsp);
+	return rsp;
+}
+
+static inline uint64_t runtime_stack_init_to_rsp(struct stack *s,
+                                                 void (*exit_fn)(void))
+{
+	uint64_t rsp;
+
 	s->usable[STACK_PTR_SIZE - 1] = (uintptr_t)exit_fn;
 	rsp = (uint64_t)&s->usable[STACK_PTR_SIZE - 1];
 	assert_rsp_aligned(rsp);
@@ -141,7 +170,7 @@ static inline uint64_t
 stack_init_to_rsp_with_buf(struct stack *s, void **buf, size_t buf_len,
 			   void (*exit_fn)(void))
 {
-	uint64_t rsp, pos = STACK_PTR_SIZE;
+	uint64_t rsp, pos = STACK_PTR_SIZE - XSAVE_AREA_PTR_SIZE;
 
 	/* reserve the buffer */
 	pos -= div_up(buf_len, sizeof(uint64_t));
@@ -363,6 +392,7 @@ BUILD_ASSERT(offsetof(struct kthread, storage_q) % CACHE_LINE_SIZE == 0);
 BUILD_ASSERT(offsetof(struct kthread, stats) % CACHE_LINE_SIZE == 0);
 
 DECLARE_PERTHREAD(struct kthread *, mykthread);
+DECLARE_PERTHREAD(void *, last_xrstor_buf);
 
 /**
  * myk - returns the per-kernel-thread data
diff --git a/runtime/kthread.c b/runtime/kthread.c
index aa949e4a..cf9918aa 100644
--- a/runtime/kthread.c
+++ b/runtime/kthread.c
@@ -37,6 +37,7 @@ atomic_t runningks;
 struct kthread *ks[NCPU];
 /* kernel thread-local data */
 DEFINE_PERTHREAD(struct kthread *, mykthread);
+DEFINE_PERTHREAD(void *, last_xrstor_buf);
 DEFINE_PERTHREAD(unsigned int, kthread_idx);
 /* Map of cpu to kthread */
 struct cpu_record cpu_map[NCPU] __attribute__((aligned(CACHE_LINE_SIZE)));
@@ -95,6 +96,8 @@ static __always_inline void kthread_yield_to_iokernel(void)
 	uint64_t last_core = k->curr_cpu;
 	ssize_t s;
 
+	perthread_store(last_xrstor_buf, NULL);
+
 	/* yield to the iokernel */
 	do {
 		clear_preempt_needed();
diff --git a/runtime/sched.c b/runtime/sched.c
index 21752588..2992eaba 100644
--- a/runtime/sched.c
+++ b/runtime/sched.c
@@ -854,6 +854,8 @@ static __always_inline thread_t *__thread_create(void)
 	th->junction_thread = false;
 	th->link_armed = false;
 	th->cur_kthread = NCPU;
+	th->xsave_area_in_use = false;
+	th->xsave_area_active = false;
 	atomic8_write(&th->interrupt_state, 0);
 
 	return th;
@@ -1036,7 +1038,7 @@ int sched_init_thread(void)
 	if (!s)
 		return -ENOMEM;
 
-	perthread_store(runtime_stack, (void *)stack_init_to_rsp(s, runtime_top_of_stack));
+	perthread_store(runtime_stack, (void *)runtime_stack_init_to_rsp(s, runtime_top_of_stack));
 	perthread_store(runtime_fsbase, _readfsbase_u64());
 
 	ss.ss_sp = &s->usable[0];
diff --git a/runtime/stack.c b/runtime/stack.c
index 787bdb7a..191832dc 100644
--- a/runtime/stack.c
+++ b/runtime/stack.c
@@ -54,7 +54,7 @@ static void stack_tcache_free(struct tcache *tc, int nr, void **items)
 
 	/* try to release the backing memory first */
 	for (i = 0; i < nr; i++)
-		stack_reclaim(container_of(items[i], struct stack, usable));
+		stack_reclaim(stack_from_tcache_handle(items[i]));
 
 	/* then make the stacks available for reallocation */
 	spin_lock(&stack_lock);
@@ -81,7 +81,7 @@ static int stack_tcache_alloc(struct tcache *tc, int nr, void **items)
 		s = stack_create();
 		if (unlikely(!s))
 			goto fail;
-		items[i] = s->usable;
+		items[i] = stack_to_tcache_handle(s);
 	}
 
 	return 0;
-- 
2.39.2

