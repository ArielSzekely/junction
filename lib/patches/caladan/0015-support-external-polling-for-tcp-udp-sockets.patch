From 4fcaf9b07277ef519459bb267cb75aec7b962d1c Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Tue, 7 Feb 2023 20:02:54 -0500
Subject: [PATCH 15/15] support external polling for tcp/udp sockets

---
 inc/runtime/poll.h   | 23 ++++++++++++++++
 inc/runtime/tcp.h    |  7 +++++
 inc/runtime/udp.h    |  4 +++
 runtime/net/tcp.c    | 52 ++++++++++++++++++++++++++++++++++++
 runtime/net/tcp.h    |  2 ++
 runtime/net/tcp_in.c | 11 ++++++--
 runtime/net/udp.c    | 63 +++++++++++++++++++++++++++++++++++++++-----
 7 files changed, 154 insertions(+), 8 deletions(-)

diff --git a/inc/runtime/poll.h b/inc/runtime/poll.h
index 8658690..86046ef 100644
--- a/inc/runtime/poll.h
+++ b/inc/runtime/poll.h
@@ -9,6 +9,29 @@
 #include <runtime/thread.h>
 #include <runtime/sync.h>
 
+// External Poll Support
+
+typedef void (*poll_notif_fn_t)(unsigned long pdata, unsigned int event_mask);
+
+typedef struct poll_source {
+	poll_notif_fn_t set_fn;
+	poll_notif_fn_t clear_fn;
+	unsigned long poller_data;
+} poll_source_t;
+
+static inline void poll_clear(poll_source_t *src, unsigned int event_mask)
+{
+	if (src->clear_fn)
+		src->clear_fn(src->poller_data, event_mask);
+}
+
+static inline void poll_set(poll_source_t *src, unsigned int event_mask)
+{
+	if (src->set_fn)
+		src->set_fn(src->poller_data, event_mask);
+}
+
+
 typedef struct poll_waiter {
 	spinlock_t		lock;
 	struct list_head	triggered;
diff --git a/inc/runtime/tcp.h b/inc/runtime/tcp.h
index 0bbb50f..e0c5482 100644
--- a/inc/runtime/tcp.h
+++ b/inc/runtime/tcp.h
@@ -5,6 +5,7 @@
 #pragma once
 
 #include <runtime/net.h>
+#include <runtime/poll.h>
 #include <sys/uio.h>
 #include <sys/socket.h>
 
@@ -32,3 +33,9 @@ extern ssize_t tcp_writev(tcpconn_t *c, const struct iovec *iov, int iovcnt);
 extern int tcp_shutdown(tcpconn_t *c, int how);
 extern void tcp_abort(tcpconn_t *c);
 extern void tcp_close(tcpconn_t *c);
+
+extern void tcp_poll_install_cb(tcpconn_t *c, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data);
+extern void tcpq_poll_install_cb(tcpqueue_t *q, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data);
+
diff --git a/inc/runtime/udp.h b/inc/runtime/udp.h
index 2d3b336..31df911 100644
--- a/inc/runtime/udp.h
+++ b/inc/runtime/udp.h
@@ -9,6 +9,7 @@
 #include <net/ip.h>
 #include <net/udp.h>
 #include <runtime/net.h>
+#include <runtime/poll.h>
 #include <sys/uio.h>
 
 /* the maximum possible payload size (for the largest possible MTU) */
@@ -48,6 +49,9 @@ extern ssize_t udp_write(udpconn_t *c, const void *buf, size_t len);
 extern void udp_shutdown(udpconn_t *c);
 extern void udp_close(udpconn_t *c);
 
+extern void udp_poll_install_cb(udpconn_t *c, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data);
+
 
 /*
  * UDP Parallel API
diff --git a/runtime/net/tcp.c b/runtime/net/tcp.c
index 9480d4d..2c23d38 100644
--- a/runtime/net/tcp.c
+++ b/runtime/net/tcp.c
@@ -3,6 +3,7 @@
  */
 
 #include <string.h>
+#include <poll.h>
 
 #include <base/stddef.h>
 #include <base/hash.h>
@@ -295,6 +296,9 @@ tcpconn_t *tcp_conn_alloc(void)
 	c->pcb.rcv_wnd = TCP_WIN;
 	c->pcb.rcv_mss = tcp_calculate_mss(net_get_mtu());
 
+	c->poll_src.set_fn = NULL;
+	c->poll_src.clear_fn = NULL;
+
 	return c;
 }
 
@@ -388,6 +392,8 @@ struct tcpqueue {
 	int			backlog;
 	bool			shutdown;
 
+	poll_source_t	poll_src;
+
 	struct kref ref;
 	struct flow_registration flow;
 };
@@ -420,6 +426,7 @@ static void tcp_queue_recv(struct trans_entry *e, struct mbuf *m)
 	spin_lock_np(&q->l);
 	list_add_tail(&q->conns, &c->queue_link);
 	th = waitq_signal(&q->wq, &q->l);
+	poll_set(&q->poll_src, POLLIN);
 	spin_unlock_np(&q->l);
 	waitq_signal_finish(th);
 
@@ -478,6 +485,9 @@ int tcp_listen(struct netaddr laddr, int backlog, tcpqueue_t **q_out)
 	q->shutdown = false;
 	kref_init(&q->ref);
 
+	q->poll_src.set_fn = NULL;
+	q->poll_src.clear_fn = NULL;
+
 	ret = trans_table_add(&q->e);
 	if (ret) {
 		sfree(q);
@@ -520,6 +530,10 @@ int tcp_accept(tcpqueue_t *q, tcpconn_t **c_out)
 	q->backlog++;
 	c = list_pop(&q->conns, tcpconn_t, queue_link);
 	assert(c != NULL);
+
+	if (list_empty(&q->conns))
+		poll_clear(&q->poll_src, POLLIN);
+
 	spin_unlock_np(&q->l);
 
 	*c_out = c;
@@ -532,6 +546,7 @@ static void __tcp_qshutdown(tcpqueue_t *q)
 	spin_lock_np(&q->l);
 	BUG_ON(q->shutdown);
 	q->shutdown = true;
+	poll_set(&q->poll_src, POLLRDHUP | POLLHUP | POLLIN);
 	spin_unlock_np(&q->l);
 
 	/* prevent ingress receive and error dispatch (after RCU period) */
@@ -784,6 +799,10 @@ static ssize_t tcp_read_wait(tcpconn_t *c, size_t len,
 		      c->tx_last_ack + c->tx_last_win + c->winmax / 4)) {
 		do_ack = true;
 	}
+
+	if (list_empty(&c->rxq))
+		poll_clear(&c->poll_src, POLLIN);
+
 	spin_unlock_np(&c->lock);
 
 	if (do_ack)
@@ -1009,6 +1028,10 @@ static void tcp_write_finish(tcpconn_t *c)
 
 	tcp_timer_update(c);
 	waitq_release_start(&c->tx_wq, &waiters);
+
+	if (tcp_is_snd_full(c))
+		poll_clear(&c->poll_src, POLLOUT);
+
 	spin_unlock_np(&c->lock);
 
 	tcp_tx_fast_retransmit_finish(c, retransmit);
@@ -1123,6 +1146,7 @@ void tcp_conn_fail(tcpconn_t *c, int err)
 	if (!c->tx_closed) {
 		c->tx_closed = true;
 		waitq_release(&c->tx_wq);
+		poll_set(&c->poll_src, POLLHUP);
 	}
 
 	/* will be freed by the writer if one is busy */
@@ -1154,6 +1178,7 @@ void tcp_conn_shutdown_rx(tcpconn_t *c)
 	if (c->rx_closed)
 		return;
 
+	poll_set(&c->poll_src, POLLRDHUP | POLLIN);
 	c->rx_closed = true;
 	waitq_release(&c->rx_wq);
 }
@@ -1180,6 +1205,7 @@ static int tcp_conn_shutdown_tx(tcpconn_t *c)
 	else
 		WARN();
 
+	poll_set(&c->poll_src, POLLHUP);
 	c->tx_closed = true;
 	waitq_release(&c->tx_wq);
 
@@ -1269,6 +1295,32 @@ void tcp_close(tcpconn_t *c)
 	tcp_conn_put(c);
 }
 
+
+void tcp_poll_install_cb(tcpconn_t *c, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data)
+{
+	spin_lock_np(&c->lock);
+	c->poll_src.set_fn = setfn;
+	c->poll_src.clear_fn = clearfn;
+	c->poll_src.poller_data = data;
+
+	if (!tcp_is_snd_full(c))
+		poll_set(&c->poll_src, POLLOUT);
+
+	spin_unlock_np(&c->lock);
+}
+
+void tcpq_poll_install_cb(tcpqueue_t *q, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data)
+{
+	spin_lock_np(&q->l);
+	q->poll_src.set_fn = setfn;
+	q->poll_src.clear_fn = clearfn;
+	q->poll_src.poller_data = data;
+	spin_unlock_np(&q->l);
+}
+
+
 /**
  * tcp_init_late - starts the TCP worker thread
  *
diff --git a/runtime/net/tcp.h b/runtime/net/tcp.h
index 48c7784..c5a4e65 100644
--- a/runtime/net/tcp.h
+++ b/runtime/net/tcp.h
@@ -91,6 +91,8 @@ struct tcpconn {
 	int			err; /* error code for read(), write(), etc. */
 	uint32_t		winmax; /* initial receive window size */
 
+	poll_source_t	poll_src;
+
 	/* ingress path */
 	unsigned int		rx_closed:1;
 	unsigned int		rx_exclusive:1;
diff --git a/runtime/net/tcp_in.c b/runtime/net/tcp_in.c
index f76a7a1..cacf73d 100644
--- a/runtime/net/tcp_in.c
+++ b/runtime/net/tcp_in.c
@@ -7,6 +7,8 @@
  * RX queue.
  */
 
+#include <poll.h>
+
 #include <base/stddef.h>
 #include <runtime/smalloc.h>
 #include <net/ip.h>
@@ -260,8 +262,10 @@ void tcp_rx_conn(struct trans_entry *e, struct mbuf *m)
 	store_release(&c->pcb.rcv_nxt_wnd, nxt_wnd);
 
 	/* should we wake a thread */
-	if (!list_empty(&c->rxq) || (tcphdr->flags & TCP_PUSH) > 0)
+	if (!list_empty(&c->rxq) || (tcphdr->flags & TCP_PUSH) > 0) {
 		rx_th = waitq_signal(&c->rx_wq, &c->lock);
+		poll_set(&c->poll_src, POLLIN);
+	}
 
 	/* handle delayed acks */
 	if (++c->acks_delayed_cnt >= 2) {
@@ -486,8 +490,10 @@ __tcp_rx_conn(tcpconn_t *c, struct mbuf *m, uint32_t ack, uint32_t snd_nxt,
 		do_ack = true;
 		goto done;
 	}
-	if (snd_was_full && !tcp_is_snd_full(c))
+	if (snd_was_full && !tcp_is_snd_full(c)) {
+		poll_set(&c->poll_src, POLLOUT);
 		waitq_release_start(&c->tx_wq, &waiters);
+	}
 
 	/*
 	 * Fast retransmit -> detect a duplicate ACK if:
@@ -548,6 +554,7 @@ __tcp_rx_conn(tcpconn_t *c, struct mbuf *m, uint32_t ack, uint32_t snd_nxt,
 			assert(!list_empty(&c->rxq));
 			assert(do_drop == false);
 			rx_th = waitq_signal(&c->rx_wq, &c->lock);
+			poll_set(&c->poll_src, POLLIN);
 		}
 		if (++c->acks_delayed_cnt >= 2) {
 			do_ack = true;
diff --git a/runtime/net/udp.c b/runtime/net/udp.c
index 8074d28..2a359ac 100644
--- a/runtime/net/udp.c
+++ b/runtime/net/udp.c
@@ -3,6 +3,7 @@
  */
 
 #include <string.h>
+#include <poll.h>
 
 #include <base/hash.h>
 #include <base/kref.h>
@@ -64,6 +65,9 @@ struct udpconn {
 	int			outq_len;
 	waitq_t			outq_wq;
 
+	/* protected by @inq_lock (less likely that POLLOUT will be cleared) */
+	poll_source_t		poll_src;
+
 	struct kref		ref;
 	struct flow_registration		flow;
 };
@@ -89,7 +93,8 @@ static void udp_conn_recv(struct trans_entry *e, struct mbuf *m)
 
 	/* enqueue the packet on the ingress queue */
 	mbufq_push_tail(&c->inq, m);
-	c->inq_len++;
+	if (c->inq_len++ == 0)
+		poll_set(&c->poll_src, POLLIN);
 
 	/* wake up a waiter */
 	th = waitq_signal(&c->inq_wq, &c->inq_lock);
@@ -108,6 +113,10 @@ static void udp_conn_err(struct trans_entry *e, int err)
 	spin_lock_np(&c->inq_lock);
 	do_release = !c->inq_err && !c->shutdown;
 	c->inq_err = err;
+
+	if (do_release)
+		poll_set(&c->poll_src, POLLERR);
+
 	spin_unlock_np(&c->inq_lock);
 
 	if (do_release)
@@ -200,6 +209,9 @@ int udp_dial(struct netaddr laddr, struct netaddr raddr, udpconn_t **c_out)
 		return ret;
 	}
 
+	c->poll_src.set_fn = NULL;
+	c->poll_src.clear_fn = NULL;
+
 	*c_out = c;
 	return 0;
 }
@@ -244,6 +256,9 @@ int udp_listen(struct netaddr laddr, udpconn_t **c_out)
 	c->flow.release = udp_release_conn_ref;
 	register_flow(&c->flow);
 
+	c->poll_src.set_fn = NULL;
+	c->poll_src.clear_fn = NULL;
+
 	*c_out = c;
 	return 0;
 }
@@ -282,6 +297,15 @@ int udp_set_buffers(udpconn_t *c, int read_mbufs, int write_mbufs)
 	c->inq_cap = read_mbufs;
 	c->outq_cap = write_mbufs;
 
+	spin_lock_np(&c->inq_lock);
+
+	if (c->outq_len < c->outq_cap)
+		poll_set(&c->poll_src, POLLOUT);
+	else
+		poll_clear(&c->poll_src, POLLOUT);
+
+	spin_unlock_np(&c->inq_lock);
+
 	/* TODO: free mbufs that go over new limits? */
 	return 0;
 }
@@ -325,7 +349,8 @@ ssize_t udp_read_from(udpconn_t *c, void *buf, size_t len,
 
 	/* pop an mbuf and deliver the payload */
 	m = mbufq_pop_head(&c->inq);
-	c->inq_len--;
+	if (--c->inq_len == 0 && !c->shutdown)
+		poll_clear(&c->poll_src, POLLIN);
 	spin_unlock_np(&c->inq_lock);
 
 	ret = MIN(len, mbuf_length(m));
@@ -351,7 +376,11 @@ static void udp_tx_release_mbuf(struct mbuf *m)
 	bool free_conn;
 
 	spin_lock_np(&c->outq_lock);
-	c->outq_len--;
+	if (c->outq_len-- == c->outq_cap) {
+		spin_lock(&c->inq_lock);
+		poll_set(&c->poll_src, POLLOUT);
+		spin_unlock(&c->inq_lock);
+	}
 	free_conn = (c->outq_free && c->outq_len == 0);
 	if (!c->shutdown)
 		th = waitq_signal(&c->outq_wq, &c->outq_lock);
@@ -409,7 +438,11 @@ ssize_t udp_write_to(udpconn_t *c, const void *buf, size_t len,
 		return -EPIPE;
 	}
 
-	c->outq_len++;
+	if (++c->outq_len >= c->outq_cap) {
+		spin_lock(&c->inq_lock);
+		poll_clear(&c->poll_src, POLLOUT);
+		spin_unlock(&c->inq_lock);
+	}
 	spin_unlock_np(&c->outq_lock);
 
 	m = net_tx_alloc_mbuf();
@@ -469,12 +502,13 @@ ssize_t udp_write(udpconn_t *c, const void *buf, size_t len)
 
 static void __udp_shutdown(udpconn_t *c)
 {
-	spin_lock_np(&c->inq_lock);
 	spin_lock_np(&c->outq_lock);
+	spin_lock(&c->inq_lock);
 	BUG_ON(c->shutdown);
 	c->shutdown = true;
+	poll_set(&c->poll_src, POLLIN | POLLHUP | POLLRDHUP);
+	spin_unlock(&c->inq_lock);
 	spin_unlock_np(&c->outq_lock);
-	spin_unlock_np(&c->inq_lock);
 
 	/* prevent ingress receive and error dispatch (after RCU period) */
 	trans_table_remove(&c->e);
@@ -522,6 +556,9 @@ void udp_close(udpconn_t *c)
 		mbuf_free(m);
 	}
 
+	c->poll_src.set_fn = NULL;
+	c->poll_src.clear_fn = NULL;
+
 	spin_lock_np(&c->outq_lock);
 	free_conn = c->outq_len == 0;
 	c->outq_free = true;
@@ -534,6 +571,20 @@ void udp_close(udpconn_t *c)
 		udp_conn_put(c);
 }
 
+void udp_poll_install_cb(udpconn_t *c, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data)
+{
+	spin_lock_np(&c->inq_lock);
+	c->poll_src.set_fn = setfn;
+	c->poll_src.clear_fn = clearfn;
+	c->poll_src.poller_data = data;
+
+	if (c->outq_len < c->outq_cap)
+		poll_set(&c->poll_src, POLLOUT);
+
+	spin_unlock_np(&c->inq_lock);
+}
+
 
 /*
  * Parallel API
-- 
2.34.1

