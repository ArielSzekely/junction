From 85cd90ab44e8de6bc0bfce658599c48c8e9eaa98 Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Thu, 28 Sep 2023 22:57:58 +0000
Subject: [PATCH 29/30] add support for interruptible waiting

---
 inc/base/atomic.h                | 10 ++++
 inc/runtime/interruptible_wait.h | 91 ++++++++++++++++++++++++++++++++
 inc/runtime/thread.h             |  8 ++-
 inc/runtime/timer.h              |  2 +
 runtime/defs.h                   |  2 +-
 runtime/interruptible_wait.c     | 12 +++++
 runtime/sched.c                  | 33 +++++++++---
 runtime/timer.c                  | 45 +++++++++++++++-
 8 files changed, 191 insertions(+), 12 deletions(-)
 create mode 100644 inc/runtime/interruptible_wait.h
 create mode 100644 runtime/interruptible_wait.c

diff --git a/inc/base/atomic.h b/inc/base/atomic.h
index 2bf2efb4..ebbad946 100644
--- a/inc/base/atomic.h
+++ b/inc/base/atomic.h
@@ -27,6 +27,11 @@ static inline int atomic_fetch_and_add(atomic_t *a, int val)
 	return __sync_fetch_and_add(&a->cnt, val);
 }
 
+static inline int atomic_fetch_and_add_relaxed(atomic_t *a, int val)
+{
+	return __atomic_fetch_add(&a->cnt, val, __ATOMIC_RELAXED);
+}
+
 static inline int atomic_fetch_and_sub(atomic_t *a, int val)
 {
 	return __sync_fetch_and_add(&a->cnt, val);
@@ -57,6 +62,11 @@ static inline int atomic_sub_and_fetch(atomic_t *a, int val)
 	return __sync_sub_and_fetch(&a->cnt, val);
 }
 
+static inline int atomic_sub_and_fetch_relaxed(atomic_t *a, int val)
+{
+	return __atomic_sub_fetch(&a->cnt, val, __ATOMIC_RELAXED);
+}
+
 static inline void atomic_inc(atomic_t *a)
 {
 	atomic_fetch_and_add(a, 1);
diff --git a/inc/runtime/interruptible_wait.h b/inc/runtime/interruptible_wait.h
new file mode 100644
index 00000000..4fc63334
--- /dev/null
+++ b/inc/runtime/interruptible_wait.h
@@ -0,0 +1,91 @@
+/*
+ * interruptible_wait.h - support for interrupting blocked threads
+ */
+
+#pragma once
+
+#include <base/list.h>
+#include <base/lock.h>
+#include <runtime/thread.h>
+
+extern bool sched_needs_signal_check(thread_t *th);
+extern void deliver_signals_jmp_thread(thread_t *th);
+
+// Returns true if this thread was interrupted.
+// @th must be thread_self().
+static inline bool prepare_interruptible(thread_t *th)
+{
+	assert(th == thread_self());
+	return atomic_fetch_and_add_relaxed(&th->interrupt_state, 1) > 0;
+}
+
+// Called after enqueuing a signal to set the interrupt flag.
+// Can only be called once, must be synchronized with signal lock.
+static inline bool deliver_interrupt(thread_t *th)
+{
+	if (atomic_fetch_and_add_relaxed(&th->interrupt_state, 1) > 0) {
+		thread_ready(th);
+		return true;
+	}
+
+	return false;
+}
+
+// Returns interrupt state.
+// After wakeup (must be synchronized using waker lock):
+// if state > 1: thread was interrupted, needs to be disarmed
+// else if state > 0: both interrupt and wake occurred, no disarm needed
+// if state == 0: thread was woken normally, no interrupt.
+static inline int get_interruptible_status(const thread_t *th)
+{
+	return atomic_read(&th->interrupt_state);
+}
+
+// Wake a thread that is blocked pending a wake or interrupt.
+// This thread must have called prepare_interruptible().
+// Must have previously synchronized with a waker lock.
+static inline void interruptible_wake_prepared(thread_t *th)
+{
+	if (atomic_sub_and_fetch_relaxed(&th->interrupt_state, 1) == 0)
+		thread_ready(th);
+}
+
+// Check if a thread was prepared to receive interrupts.
+// This can only be called by a waker, which must have previously synchronized
+// with a waker lock.
+static inline bool check_prepared(const thread_t *th) {
+	return atomic_read(&th->interrupt_state) != 0;
+}
+
+// Wake a thread that is blocked.
+// The thread does not need to have been armed with prepare_interruptible().
+// If the caller is certain that this thread was armed, it can call
+// interruptible_wake_prepared() directly.
+// Must have previously synchronized with a waker lock.
+static inline void interruptible_wake(thread_t *th)
+{
+	if (!check_prepared(th) ||
+		atomic_sub_and_fetch_relaxed(&th->interrupt_state, 1) == 0)
+		thread_ready(th);
+}
+
+// Must be called with signal lock to synchronize with interrupt delivery.
+static inline void reset_interruptible_state(thread_t *th)
+{
+	atomic_write(&th->interrupt_state, 0);
+}
+
+// Mark this thread interrupted, typically used when the blocked signal mask
+// is updated. Caller should synchronize with signal lock.
+static inline void set_interrupt_state_interrupted()
+{
+	atomic_write(&thread_self()->interrupt_state, 1);
+}
+
+// Check if a thread was interrupted.
+// @th must be thread_self().
+static inline bool thread_interrupted(const thread_t *th)
+{
+	assert(th == thread_self());
+	return atomic_read(&th->interrupt_state) > 0;
+}
diff --git a/inc/runtime/thread.h b/inc/runtime/thread.h
index e4867405..6e52f997 100644
--- a/inc/runtime/thread.h
+++ b/inc/runtime/thread.h
@@ -106,12 +106,16 @@ struct thread {
     unsigned int        thread_ready;
     unsigned int        thread_running;
     unsigned int        last_cpu;
+    unsigned int        cur_kthread;
     uint64_t        run_start_tsc;
     uint64_t        ready_tsc;
     uint64_t        tlsvar;
-     // Trapframe used by junction to stash registers on syscall entry
-    struct thread_tf	junction_tf;
+    atomic_t        interrupt_state;
     unsigned long    junction_tstate_buf[24];
+
+    // Trapframe used by junction to stash registers on syscall entry
+    struct thread_tf    junction_tf;
+
 #ifdef GC
     struct list_node    gc_link;
     unsigned int        onk;
diff --git a/inc/runtime/timer.h b/inc/runtime/timer.h
index 1c11df25..0c7d1e79 100644
--- a/inc/runtime/timer.h
+++ b/inc/runtime/timer.h
@@ -92,3 +92,5 @@ extern bool timer_cancel_recurring(struct timer_entry *e);
 
 extern void timer_sleep_until(uint64_t deadline_us);
 extern void timer_sleep(uint64_t duration_us);
+extern void __timer_sleep_interruptible(uint64_t deadline_us);
+extern void timer_sleep_interruptible(uint64_t duration_us);
\ No newline at end of file
diff --git a/runtime/defs.h b/runtime/defs.h
index a0c94f9d..fbc220e2 100644
--- a/runtime/defs.h
+++ b/runtime/defs.h
@@ -46,7 +46,7 @@
 typedef void (*runtime_fn_t)(void);
 
 /* assembly helper routines from switch.S */
-extern void __jmp_thread(struct thread_tf *tf) __noreturn;
+extern void __jmp_thread(struct thread_tf *tf);
 extern void __jmp_thread_direct(struct thread_tf *oldtf,
 				struct thread_tf *newtf,
 				unsigned int *thread_running);
diff --git a/runtime/interruptible_wait.c b/runtime/interruptible_wait.c
new file mode 100644
index 00000000..901ef843
--- /dev/null
+++ b/runtime/interruptible_wait.c
@@ -0,0 +1,12 @@
+/*
+ * interruptible_wait.c - support for interrupting blocked threads
+ */
+
+#include <runtime/sync.h>
+#include <runtime/interruptible_wait.h>
+
+#include "defs.h"
+
+// Junction overrides these symbols
+bool __weak sched_needs_signal_check(thread_t *th) { return false; }
+void __weak deliver_signals_jmp_thread(thread_t *th) {}
\ No newline at end of file
diff --git a/runtime/sched.c b/runtime/sched.c
index 7a90ec3f..70d8c46a 100644
--- a/runtime/sched.c
+++ b/runtime/sched.c
@@ -16,6 +16,7 @@
 #include <base/log.h>
 #include <runtime/sync.h>
 #include <runtime/thread.h>
+#include <runtime/interruptible_wait.h>
 
 #include "defs.h"
 
@@ -72,7 +73,7 @@ static inline bool cores_have_affinity(unsigned int cpua, unsigned int cpub)
  * This function restores the state of the thread and switches from the runtime
  * stack to the thread's stack. Runtime state is not saved.
  */
-static __noreturn void jmp_thread(thread_t *th)
+static void jmp_thread(thread_t *th)
 {
 	assert_preempt_disabled();
 	assert(th->thread_ready);
@@ -91,6 +92,10 @@ static __noreturn void jmp_thread(thread_t *th)
 	set_fsbase(th->tf.fsbase);
 
 	th->thread_running = true;
+
+	if (unlikely(sched_needs_signal_check(th)))
+		deliver_signals_jmp_thread(th);
+
 	__jmp_thread(&th->tf);
 }
 
@@ -328,11 +333,11 @@ static __noinline bool do_watchdog(struct kthread *l)
 }
 
 /* the main scheduler routine, decides what to run next */
-static __noreturn __noinline void schedule(void)
+static __noinline void schedule(void)
 {
 	struct kthread *r = NULL, *l = myk();
 	uint64_t start_tsc;
-	thread_t *th = NULL;
+	thread_t *th;
 	unsigned int start_idx;
 	unsigned int iters = 0;
 	int i, sibling;
@@ -343,10 +348,14 @@ static __noreturn __noinline void schedule(void)
 	/* detect misuse of preempt disable */
 	BUG_ON((perthread_read(preempt_cnt) & ~PREEMPT_NOT_PENDING) != 1);
 
+	th = perthread_get_stable(__self);
+
 	/* unmark busy for the stack of the last uthread */
-	if (likely(perthread_get_stable(__self) != NULL)) {
-		store_release(&perthread_get_stable(__self)->thread_running, false);
+	if (likely(th != NULL)) {
+		store_release(&th->thread_running, false);
+		th->cur_kthread = NCPU;
 		perthread_get_stable(__self) = NULL;
+		th = NULL;
 	}
 
 	/* update entry stat counters */
@@ -461,6 +470,9 @@ done:
 		drain_overflow(l);
 
 	update_oldest_tsc(l);
+
+	th->cur_kthread = l->kthread_idx;
+
 	spin_unlock(&l->lock);
 
 	/* update exit stat counters */
@@ -498,9 +510,12 @@ static __always_inline void enter_schedule(thread_t *curth)
 	spin_lock(&k->lock);
 	now_tsc = rdtsc();
 
+	th = k->rq[k->rq_tail % RUNTIME_RQ_SIZE];
+
 	/* slow path: switch from the uthread stack to the runtime stack */
 	if (k->rq_head == k->rq_tail ||
 	    preempt_cede_needed(k) ||
+	    sched_needs_signal_check(th) ||
 #ifdef GC
 	    get_gc_gen() != k->local_gc_gen ||
 #endif
@@ -516,7 +531,7 @@ static __always_inline void enter_schedule(thread_t *curth)
 	perthread_get_stable(last_tsc) = now_tsc;
 
 	/* pop the next runnable thread from the queue */
-	th = k->rq[k->rq_tail++ % RUNTIME_RQ_SIZE];
+	k->rq_tail++;
 	ACCESS_ONCE(k->q_ptrs->rq_tail)++;
 
 	/* move overflow tasks into the runqueue */
@@ -524,6 +539,8 @@ static __always_inline void enter_schedule(thread_t *curth)
 		drain_overflow(k);
 
 	update_oldest_tsc(k);
+	curth->cur_kthread = NCPU;
+	th->cur_kthread = k->kthread_idx;
 	spin_unlock(&k->lock);
 
 	/* update exported thread run start time */
@@ -834,6 +851,8 @@ static __always_inline thread_t *__thread_create(void)
 	th->thread_ready = false;
 	th->thread_running = false;
 	th->tlsvar = 0;
+	th->cur_kthread = NCPU;
+	atomic_write(&th->interrupt_state, 0);
 
 	return th;
 }
@@ -963,7 +982,7 @@ void thread_exit(void)
  * immediately park each kthread when it first starts up, only schedule it once
  * the iokernel has granted it a core
  */
-static __noreturn void schedule_start(void)
+static void schedule_start(void)
 {
 	struct kthread *k = myk();
 
diff --git a/runtime/timer.c b/runtime/timer.c
index 9c3ae258..75d25b10 100644
--- a/runtime/timer.c
+++ b/runtime/timer.c
@@ -12,6 +12,7 @@
 #include <runtime/sync.h>
 #include <runtime/thread.h>
 #include <runtime/timer.h>
+#include <runtime/interruptible_wait.h>
 
 #include "defs.h"
 
@@ -268,8 +269,7 @@ static void __timer_sleep(uint64_t deadline_us)
 	timer_init(&e, timer_finish_sleep, (unsigned long)thread_self());
 
 	k = getk();
-	spin_lock_np(&k->timer_lock);
-	putk();
+	spin_lock(&k->timer_lock);
 	timer_start_locked(k, &e, deadline_us);
 	update_q_ptrs(k);
 	thread_park_and_unlock_np(&k->timer_lock);
@@ -277,6 +277,38 @@ static void __timer_sleep(uint64_t deadline_us)
 	timer_finish(&e);
 }
 
+static void timer_finish_interruptible_sleep(unsigned long arg)
+{
+	thread_t *th = (thread_t *)arg;
+	interruptible_wake_prepared(th);
+}
+
+
+void __timer_sleep_interruptible(uint64_t deadline_us)
+{
+	struct kthread *k;
+	struct timer_entry e;
+
+	thread_t *th = thread_self();
+
+	timer_init(&e, timer_finish_interruptible_sleep, (unsigned long)th);
+
+	k = getk();
+
+	spin_lock(&k->timer_lock);
+
+	if (prepare_interruptible(th)) {
+		spin_unlock_np(&k->timer_lock);
+		return;
+	}
+
+	timer_start_locked(k, &e, deadline_us);
+	update_q_ptrs(k);
+	thread_park_and_unlock_np(&k->timer_lock);
+
+	timer_cancel(&e);
+}
+
 /**
  * timer_sleep_until - sleeps until a deadline
  * @deadline_us: the deadline time in microseconds
@@ -298,6 +330,15 @@ void timer_sleep(uint64_t duration_us)
 	__timer_sleep(microtime() + duration_us);
 }
 
+/**
+ * timer_sleep - sleeps for a duration
+ * @duration_us: the duration time in microseconds
+ */
+void timer_sleep_interruptible(uint64_t duration_us)
+{
+	__timer_sleep_interruptible(microtime() + duration_us);
+}
+
 static void timer_softirq_one(struct kthread *k)
 {
 	struct timer_entry *e;
-- 
2.39.2

