From e86d2a4c1eea1dc67dc9a3fea7db5963adca2515 Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Wed, 4 Oct 2023 21:33:11 +0000
Subject: [PATCH 31/32] make TCP/UDP sockets interruptible

---
 runtime/net/tcp.c    | 79 ++++++++++++++++++++++++++++++--------------
 runtime/net/tcp.h    |  4 +--
 runtime/net/tcp_in.c |  2 +-
 runtime/net/udp.c    | 43 +++++++++++++++---------
 runtime/net/waitq.h  | 68 ++++++++++++++++++++++++++++++--------
 5 files changed, 139 insertions(+), 57 deletions(-)

diff --git a/runtime/net/tcp.c b/runtime/net/tcp.c
index 035df38c..e66e756c 100644
--- a/runtime/net/tcp.c
+++ b/runtime/net/tcp.c
@@ -171,7 +171,7 @@ void tcp_free_rx_bufs(void)
 			continue;
 
 		spin_lock_np(&c->lock);
-		waitq_release_start(&c->rx_wq, &waiters);
+		waitq_release_start(&c->rx_wq, &waiters, &c->lock);
 		list_append_list(&mbufs, &c->rxq_ooo);
 		c->rxq_ooo_len = 0;
 		spin_unlock_np(&c->lock);
@@ -231,7 +231,7 @@ void tcp_conn_set_state(tcpconn_t *c, int new_state)
 	/* unblock any threads waiting for the connection to be established */
 	if (c->pcb.state < TCP_STATE_ESTABLISHED &&
 	    new_state >= TCP_STATE_ESTABLISHED) {
-		waitq_release(&c->tx_wq);
+		waitq_release(&c->tx_wq, &c->lock);
 		poll_set(&c->poll_src, POLLOUT);
 	}
 
@@ -572,6 +572,7 @@ int tcp_listen(struct netaddr laddr, int backlog, tcpqueue_t **q_out)
  */
 int tcp_accept(tcpqueue_t *q, tcpconn_t **c_out)
 {
+	int ret;
 	tcpconn_t *c;
 
 	spin_lock_np(&q->l);
@@ -581,7 +582,11 @@ int tcp_accept(tcpqueue_t *q, tcpconn_t **c_out)
 			spin_unlock_np(&q->l);
 			return -EAGAIN;
 		}
-		waitq_wait(&q->wq, &q->l);
+		ret = waitq_wait(&q->wq, &q->l);
+		if (unlikely(ret)) {
+			spin_unlock_np(&q->l);
+			return ret;
+		}
 	}
 
 	/* was the queue drained and shutdown? */
@@ -611,6 +616,10 @@ static void __tcp_qshutdown(tcpqueue_t *q)
 	BUG_ON(q->shutdown);
 	q->shutdown = true;
 	poll_set(&q->poll_src, POLLRDHUP | POLLHUP | POLLIN);
+
+	/* wake up all pending threads */
+	waitq_release(&q->wq, &q->l);
+
 	spin_unlock_np(&q->l);
 
 	/* prevent ingress receive and error dispatch (after RCU period) */
@@ -629,9 +638,6 @@ void tcp_qshutdown(tcpqueue_t *q)
 {
 	/* shutdown the listen queue */
 	__tcp_qshutdown(q);
-
-	/* wake up all pending threads */
-	waitq_release(&q->wq);
 }
 
 /**
@@ -717,8 +723,13 @@ static int __tcp_dial(struct netaddr laddr, struct netaddr raddr,
 	}
 
 	/* wait until the connection is established or there is a failure */
-	while (!c->tx_closed && c->pcb.state < TCP_STATE_ESTABLISHED)
-		waitq_wait(&c->tx_wq, &c->lock);
+	while (!c->tx_closed && c->pcb.state < TCP_STATE_ESTABLISHED) {
+		ret = waitq_wait(&c->tx_wq, &c->lock);
+		if (!c->tx_closed && ret) {
+			spin_unlock_np(&c->lock);
+			return ret;
+		}
+	}
 
 	/* check if the connection failed */
 	if (c->tx_closed) {
@@ -849,6 +860,7 @@ struct netaddr tcp_remote_addr(tcpconn_t *c)
 static ssize_t tcp_read_wait(tcpconn_t *c, size_t len,
 			     struct list_head *q, struct mbuf **mout)
 {
+	int ret;
 	struct mbuf *m;
 	size_t readlen = 0;
 	bool do_ack = false;
@@ -862,7 +874,11 @@ static ssize_t tcp_read_wait(tcpconn_t *c, size_t len,
 			spin_unlock_np(&c->lock);
 			return -EAGAIN;
 		}
-		waitq_wait(&c->rx_wq, &c->lock);
+		ret = waitq_wait(&c->rx_wq, &c->lock);
+		if (unlikely(ret)) {
+			spin_unlock_np(&c->lock);
+			return ret;
+		}
 	}
 
 	/* is the socket closed? */
@@ -921,7 +937,7 @@ static void tcp_read_finish(tcpconn_t *c, struct mbuf *m)
 	list_head_init(&waiters);
 	spin_lock_np(&c->lock);
 	c->rx_exclusive = false;
-	waitq_release_start(&c->rx_wq, &waiters);
+	waitq_release_start(&c->rx_wq, &waiters, &c->lock);
 	spin_unlock_np(&c->lock);
 	waitq_release_finish(&waiters);
 }
@@ -1065,6 +1081,7 @@ ssize_t tcp_readv(tcpconn_t *c, const struct iovec *iov, int iovcnt)
 
 static int tcp_write_wait(tcpconn_t *c, size_t *winlen)
 {
+	int ret;
 	spin_lock_np(&c->lock);
 
 	/* block until there is an actionable event */
@@ -1081,7 +1098,11 @@ static int tcp_write_wait(tcpconn_t *c, size_t *winlen)
 			spin_unlock_np(&c->lock);
 			return -EAGAIN;
 		}
-		waitq_wait(&c->tx_wq, &c->lock);
+		ret = waitq_wait(&c->tx_wq, &c->lock);
+		if (unlikely(ret)) {
+			spin_unlock_np(&c->lock);
+			return ret;
+		}
 	}
 	c->zero_wnd = false;
 
@@ -1132,7 +1153,7 @@ static void tcp_write_finish(tcpconn_t *c)
 	}
 
 	tcp_timer_update(c);
-	waitq_release_start(&c->tx_wq, &waiters);
+	waitq_release_start(&c->tx_wq, &waiters, &c->lock);
 
 	if (tcp_is_snd_full(c))
 		poll_clear(&c->poll_src, POLLOUT);
@@ -1218,7 +1239,7 @@ static void tcp_retransmit(void *arg)
 	spin_lock_np(&c->lock);
 
 	while (c->tx_exclusive && c->pcb.state != TCP_STATE_CLOSED)
-		waitq_wait(&c->tx_wq, &c->lock);
+		waitq_wait_uninterruptible(&c->tx_wq, &c->lock);
 
 	if (c->pcb.state != TCP_STATE_CLOSED) {
 		c->tx_exclusive = true;
@@ -1250,7 +1271,7 @@ void tcp_conn_fail(tcpconn_t *c, int err)
 
 	if (!c->tx_closed) {
 		store_release(&c->tx_closed, true);
-		waitq_release(&c->tx_wq);
+		waitq_release(&c->tx_wq, &c->lock);
 		poll_set(&c->poll_src, POLLHUP);
 	}
 
@@ -1285,10 +1306,10 @@ void tcp_conn_shutdown_rx(tcpconn_t *c)
 
 	poll_set(&c->poll_src, POLLRDHUP | POLLIN);
 	c->rx_closed = true;
-	waitq_release(&c->rx_wq);
+	waitq_release(&c->rx_wq, &c->lock);
 }
 
-static int tcp_conn_shutdown_tx(tcpconn_t *c)
+static int tcp_conn_shutdown_tx(tcpconn_t *c, bool interruptible)
 {
 	int ret;
 
@@ -1303,8 +1324,16 @@ static int tcp_conn_shutdown_tx(tcpconn_t *c)
 		return 0;
 	}
 
-	while (c->tx_exclusive)
-		waitq_wait(&c->tx_wq, &c->lock);
+	while (c->tx_exclusive) {
+		if (interruptible) {
+			ret = waitq_wait(&c->tx_wq, &c->lock);
+			if (ret)
+				return ret;
+		} else {
+			waitq_wait_uninterruptible(&c->tx_wq, &c->lock);
+		}
+	}
+
 	ret = tcp_tx_ctl(c, TCP_FIN | TCP_ACK, NULL);
 	if (unlikely(ret))
 		return ret;
@@ -1317,7 +1346,7 @@ static int tcp_conn_shutdown_tx(tcpconn_t *c)
 
 	poll_set(&c->poll_src, POLLHUP);
 	c->tx_closed = true;
-	waitq_release(&c->tx_wq);
+	waitq_release(&c->tx_wq, &c->lock);
 
 	return 0;
 }
@@ -1342,7 +1371,7 @@ int tcp_shutdown(tcpconn_t *c, int how)
 
 	spin_lock_np(&c->lock);
 	if (tx) {
-		ret = tcp_conn_shutdown_tx(c);
+		ret = tcp_conn_shutdown_tx(c, true);
 		if (ret) {
 			spin_unlock_np(&c->lock);
 			return ret;
@@ -1375,7 +1404,7 @@ void tcp_abort(tcpconn_t *c)
 	tcp_conn_fail(c, ECONNABORTED);
 
 	while (c->tx_exclusive)
-		waitq_wait(&c->tx_wq, &c->lock);
+		waitq_wait_uninterruptible(&c->tx_wq, &c->lock);
 
 	snd_nxt = c->pcb.snd_nxt;
 	spin_unlock_np(&c->lock);
@@ -1396,7 +1425,7 @@ void tcp_close(tcpconn_t *c)
 
 	spin_lock_np(&c->lock);
 	BUG_ON(!waitq_empty(&c->rx_wq));
-	ret = tcp_conn_shutdown_tx(c);
+	ret = tcp_conn_shutdown_tx(c, false);
 	if (ret)
 		tcp_conn_fail(c, -ret);
 	tcp_conn_shutdown_rx(c);
@@ -1414,8 +1443,8 @@ void tcp_set_nonblocking(tcpconn_t *c, bool nonblocking)
 	spin_lock_np(&c->lock);
 	c->nonblocking = nonblocking;
 	if (nonblocking) {
-		waitq_release(&c->tx_wq);
-		waitq_release(&c->rx_wq);
+		waitq_release(&c->tx_wq, &c->lock);
+		waitq_release(&c->rx_wq, &c->lock);
 	}
 	spin_unlock_np(&c->lock);
 }
@@ -1425,7 +1454,7 @@ void tcpq_set_nonblocking(tcpqueue_t *q, bool nonblocking)
 	spin_lock_np(&q->l);
 	q->nonblocking = nonblocking;
 	if (nonblocking)
-		waitq_release(&q->wq);
+		waitq_release(&q->wq, &q->l);
 	spin_unlock_np(&q->l);
 }
 
diff --git a/runtime/net/tcp.h b/runtime/net/tcp.h
index 625281bd..0c87d75f 100644
--- a/runtime/net/tcp.h
+++ b/runtime/net/tcp.h
@@ -6,11 +6,11 @@
 #include <base/list.h>
 #include <base/kref.h>
 #include <base/time.h>
-#include <runtime/sync.h>
-#include <runtime/tcp.h>
 #include <net/tcp.h>
 #include <net/mbuf.h>
 #include <net/mbufq.h>
+#include <runtime/sync.h>
+#include <runtime/tcp.h>
 
 #include "defs.h"
 #include "waitq.h"
diff --git a/runtime/net/tcp_in.c b/runtime/net/tcp_in.c
index 0499c1d9..9f5bbec4 100644
--- a/runtime/net/tcp_in.c
+++ b/runtime/net/tcp_in.c
@@ -498,7 +498,7 @@ __tcp_rx_conn(tcpconn_t *c, struct mbuf *m, uint32_t ack, uint32_t snd_nxt,
 	}
 	if (snd_was_full && !tcp_is_snd_full(c)) {
 		poll_set(&c->poll_src, POLLOUT);
-		waitq_release_start(&c->tx_wq, &waiters);
+		waitq_release_start(&c->tx_wq, &waiters, &c->lock);
 	}
 
 	/*
diff --git a/runtime/net/udp.c b/runtime/net/udp.c
index 153df4b7..605381e8 100644
--- a/runtime/net/udp.c
+++ b/runtime/net/udp.c
@@ -109,19 +109,17 @@ static void udp_conn_err(struct trans_entry *e, int err)
 {
 	udpconn_t *c = container_of(e, udpconn_t, e);
 
-	bool do_release;
-
 	spin_lock_np(&c->inq_lock);
-	do_release = !c->inq_err && !c->shutdown;
-	c->inq_err = err;
 
-	if (do_release)
+	if (!c->inq_err && !c->shutdown) {
 		poll_set(&c->poll_src, POLLERR);
+		waitq_release(&c->inq_wq, &c->inq_lock);
+	}
+
+	c->inq_err = err;
 
 	spin_unlock_np(&c->inq_lock);
 
-	if (do_release)
-		waitq_release(&c->inq_wq);
 }
 
 /* operations for UDP sockets */
@@ -336,7 +334,11 @@ ssize_t udp_read_from(udpconn_t *c, void *buf, size_t len,
 			spin_unlock_np(&c->inq_lock);
 			return -EAGAIN;
 		}
-		waitq_wait(&c->inq_wq, &c->inq_lock);
+		ret = waitq_wait(&c->inq_wq, &c->inq_lock);
+		if (unlikely(ret)) {
+			spin_unlock_np(&c->inq_lock);
+			return ret;
+		}
 	}
 
 	/* is the socket drained and shutdown? */
@@ -438,7 +440,11 @@ ssize_t udp_write_to(udpconn_t *c, const void *buf, size_t len,
 			spin_unlock_np(&c->outq_lock);
 			return -EAGAIN;
 		}
-		waitq_wait(&c->outq_wq, &c->outq_lock);
+		ret = waitq_wait(&c->outq_wq, &c->outq_lock);
+		if (unlikely(ret)) {
+			spin_unlock_np(&c->outq_lock);
+			return ret;
+		}
 	}
 
 	/* is the socket shutdown? */
@@ -511,16 +517,26 @@ ssize_t udp_write(udpconn_t *c, const void *buf, size_t len)
 
 static void __udp_shutdown(udpconn_t *c)
 {
+	LIST_HEAD(waiters);
+
 	spin_lock_np(&c->outq_lock);
 	spin_lock(&c->inq_lock);
 	BUG_ON(c->shutdown);
 	c->shutdown = true;
 	poll_set(&c->poll_src, POLLIN | POLLHUP | POLLRDHUP);
+
+	/* wake all blocked threads */
+	if (!c->inq_err)
+		waitq_release_start(&c->inq_wq, &waiters, &c->inq_lock);
+	waitq_release_start(&c->outq_wq, &waiters ,&c->outq_lock);
+
 	spin_unlock(&c->inq_lock);
 	spin_unlock_np(&c->outq_lock);
 
 	/* prevent ingress receive and error dispatch (after RCU period) */
 	trans_table_remove(&c->e);
+
+	waitq_release_finish(&waiters);
 }
 
 /**
@@ -533,11 +549,6 @@ void udp_shutdown(udpconn_t *c)
 {
 	/* shutdown the UDP socket */
 	__udp_shutdown(c);
-
-	/* wake all blocked threads */
-	if (!c->inq_err)
-		waitq_release(&c->inq_wq);
-	waitq_release(&c->outq_wq);
 }
 
 /**
@@ -586,8 +597,8 @@ void udp_set_nonblocking(udpconn_t *c, bool nonblocking)
 	spin_lock(&c->inq_lock);
 	c->nonblocking = nonblocking;
 	if (nonblocking) {
-		waitq_release(&c->inq_wq);
-		waitq_release(&c->outq_wq);
+		waitq_release(&c->inq_wq, &c->inq_lock);
+		waitq_release(&c->outq_wq, &c->outq_lock);
 	}
 	spin_unlock(&c->inq_lock);
 	spin_unlock_np(&c->outq_lock);
diff --git a/runtime/net/waitq.h b/runtime/net/waitq.h
index 17fed7f9..fd5675b1 100644
--- a/runtime/net/waitq.h
+++ b/runtime/net/waitq.h
@@ -8,17 +8,20 @@
 #include <base/list.h>
 #include <runtime/thread.h>
 #include <runtime/sync.h>
+#include <runtime/interruptible_wait.h>
+
+#define RESTART_ERROR 512 /* ERESTARTSYS */
 
 typedef struct waitq {
 	struct list_head	waiters;
 } waitq_t;
 
 /**
- * waitq_wait - waits for the next signal
+ * waitq_wait_uninterruptible - waits for the next signal
  * @q: the wake queue
  * @l: a held spinlock protecting the wake queue and the condition
  */
-static inline void waitq_wait(waitq_t *q, spinlock_t *l)
+static inline void waitq_wait_uninterruptible(waitq_t *q, spinlock_t *l)
 {
 	assert_spin_lock_held(l);
 	list_add_tail(&q->waiters, &thread_self()->link);
@@ -26,6 +29,32 @@ static inline void waitq_wait(waitq_t *q, spinlock_t *l)
 	spin_lock_np(l);
 }
 
+/**
+ * waitq_wait - waits for the next signal
+ * @q: the wake queue
+ * @l: a held spinlock protecting the wake queue and the condition
+ *
+ * Returns 0 if succeeded, or -ERESTARTSYS if interrupted
+ */
+static inline __must_use_return int waitq_wait(waitq_t *q, spinlock_t *l)
+{
+	thread_t *myth = thread_self();
+	assert_spin_lock_held(l);
+
+	if (prepare_interruptible(myth))
+		return -RESTART_ERROR;
+
+	list_add_tail(&q->waiters, &myth->interruptible_link);
+	thread_park_and_unlock_np(l);
+	spin_lock_np(l);
+
+	int status = get_interruptible_status(myth);
+	if (unlikely(status > 1))
+		list_del_from(&q->waiters, &myth->interruptible_link);
+
+	return status > 0 ? -RESTART_ERROR : 0;
+}
+
 /**
  * waitq_signal - wakes up to one waiter on the wake queue
  * @q: the wake queue
@@ -34,7 +63,10 @@ static inline void waitq_wait(waitq_t *q, spinlock_t *l)
 static inline thread_t *waitq_signal(waitq_t *q, spinlock_t *l)
 {
 	assert_spin_lock_held(l);
-	return list_pop(&q->waiters, thread_t, link);
+	thread_t *th = list_pop(&q->waiters, thread_t, interruptible_link);
+	if (!th || !interruptible_wake_test(th))
+		return NULL;
+	return th;
 }
 
 /**
@@ -46,8 +78,10 @@ static inline thread_t *waitq_signal(waitq_t *q, spinlock_t *l)
  */
 static inline void waitq_signal_finish(thread_t *th)
 {
-	if (th)
+	if (th) {
+		assert(!check_prepared(th));
 		thread_ready(th);
+	}
 }
 
 /**
@@ -65,31 +99,39 @@ static inline void waitq_signal_locked(waitq_t *q, spinlock_t *l)
  * waitq_release - wakes all pending waiters
  * @q: the wake queue
  *
- * WARNING: the condition must have been updated with the lock held to
- * prevent future waiters. However, this method can be called after the
- * lock is released.
  */
-static inline void waitq_release(waitq_t *q)
+static inline void waitq_release(waitq_t *q, spinlock_t *l)
 {
+	assert_spin_lock_held(l);
 	while (true) {
-		thread_t *th = list_pop(&q->waiters, thread_t, link);
+		thread_t *th = list_pop(&q->waiters, thread_t, interruptible_link);
 		if (!th)
 			break;
-		thread_ready(th);
+		interruptible_wake(th);
 	}
 }
 
-static inline void waitq_release_start(waitq_t *q, struct list_head *waiters)
+static inline void waitq_release_start(waitq_t *q, struct list_head *waiters,
+	                                   spinlock_t *l)
 {
-	list_append_list(waiters, &q->waiters);
+	assert_spin_lock_held(l);
+
+	while (true) {
+		thread_t *th = list_pop(&q->waiters, thread_t, interruptible_link);
+		if (!th)
+			break;
+		if (interruptible_wake_test(th))
+			list_add_tail(waiters, &th->interruptible_link);
+	}
 }
 
 static inline void waitq_release_finish(struct list_head *waiters)
 {
 	while (true) {
-		thread_t *th = list_pop(waiters, thread_t, link);
+		thread_t *th = list_pop(waiters, thread_t, interruptible_link);
 		if (!th)
 			break;
+		assert(!check_prepared(th));
 		thread_ready(th);
 	}
 }
-- 
2.39.2

