From dc83327dd47494744d7001fe33330cb2d80d2537 Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Mon, 24 Jul 2023 18:18:59 +0000
Subject: [PATCH 27/28] iokernel: better struct packing

---
 iokernel/control.c           |  5 +-
 iokernel/defs.h              | 78 ++++++++++++++++++--------------
 iokernel/directpath/queues.c | 14 ++++--
 iokernel/sched.c             | 88 ++++++++++++++++++++----------------
 4 files changed, 105 insertions(+), 80 deletions(-)

diff --git a/iokernel/control.c b/iokernel/control.c
index 17f8360..4ab30d4 100644
--- a/iokernel/control.c
+++ b/iokernel/control.c
@@ -282,8 +282,8 @@ static struct proc *control_create_proc(mem_key_t key, size_t len,
 
 		th->tid = s->tid;
 		th->p = p;
-		th->at_idx = UINT_MAX;
-		th->ts_idx = UINT_MAX;
+		th->at_idx = UINT16_MAX;
+		th->ts_idx = UINT16_MAX;
 
 		/* initialize pointer to queue pointers in shared memory */
 		th->q_ptrs = (struct q_ptrs *) shmptr_to_ptr(&reg, s->q_ptrs,
@@ -300,6 +300,7 @@ static struct proc *control_create_proc(mem_key_t key, size_t len,
 			goto fail;
 
 		p->has_directpath |= th->directpath_hwq.enabled;
+		p->has_storage |= th->storage_hwq.enabled;
 	}
 
 	/* initialize the table of physical page addresses */
diff --git a/iokernel/defs.h b/iokernel/defs.h
index d057af3..ff145c5 100644
--- a/iokernel/defs.h
+++ b/iokernel/defs.h
@@ -42,7 +42,7 @@ extern uint32_t nr_vfio_prealloc;
  * Constant limits
  */
 
-#define IOKERNEL_MAX_PROC		2048
+#define IOKERNEL_MAX_PROC		4096
 #define IOKERNEL_NUM_MBUFS		(8192 * 16)
 #define IOKERNEL_NUM_COMPLETIONS	32767
 #define IOKERNEL_OVERFLOW_BATCH_DRAIN	64
@@ -52,6 +52,9 @@ extern uint32_t nr_vfio_prealloc;
 #define IOKERNEL_CONTROL_BURST_SIZE	4
 #define IOKERNEL_POLL_INTERVAL		10
 
+/* Ensure that uint16_t can be used to index procs/cores */
+BUILD_ASSERT(NCPU < UINT16_MAX);
+BUILD_ASSERT(IOKERNEL_MAX_PROC < UINT16_MAX);
 
 /*
  * Process Support
@@ -82,31 +85,32 @@ struct thread_metrics {
 
 struct thread {
 	bool			active;
+	uint16_t		core;
 	pid_t			tid;
 	uint64_t		next_poll_tsc;
-	struct proc		*p;
-	struct lrpc_chan_out	rxq;
-	struct lrpc_chan_in	txpktq;
-	struct lrpc_chan_in	txcmdq;
 	struct q_ptrs		*q_ptrs;
+	struct proc 	*p;
+	struct list_node	idle_link;
+
+	/* useful metrics for scheduling policies */
+	struct thread_metrics	metrics;
+
+	uint64_t		rxq_busy_since;
+	uint64_t		wake_gen;
+	uint64_t		change_tsc;
 	uint32_t		last_rq_head;
 	uint32_t		last_rq_tail;
 	uint32_t		last_rxq_head;
 	uint32_t		last_rxq_tail;
-	uint64_t		rxq_busy_since;
-	unsigned int		core;
-	unsigned int		at_idx;
-	unsigned int		ts_idx;
 	uint32_t		last_yield_rcu_gen;
-	uint64_t		wake_gen;
-	uint64_t		change_tsc;
 
+	struct lrpc_chan_out	rxq;
+	struct lrpc_chan_in	txpktq;
+	struct lrpc_chan_in	txcmdq;
 	struct hwq		directpath_hwq;
 	struct hwq		storage_hwq;
-	struct list_node	idle_link;
-
-	/* useful metrics for scheduling policies */
-	struct thread_metrics	metrics;
+	uint16_t		at_idx;
+	uint16_t		ts_idx;
 };
 
 static inline void thread_enable_sched_poll(struct thread *th)
@@ -143,11 +147,10 @@ static inline bool hwq_busy(struct hwq *h, uint32_t cq_idx)
 }
 
 struct proc {
-	pid_t			pid;
-	struct shm_region	region;
-
-	struct ref		ref;
-
+	/* hot data */
+	struct ref			ref;
+	uint16_t			thread_count;
+	uint16_t			active_thread_count;
 	unsigned int		has_directpath:1;
 	unsigned int		has_vfio_directpath:1;
 	unsigned int		vfio_directpath_rmp:1;
@@ -155,36 +158,41 @@ struct proc {
 	unsigned int		attach_fail:1;
 	unsigned int		removed:1;
 	unsigned int		started:1;
+	unsigned int		has_storage:1;
 	struct runtime_info	*runtime_info;
 	unsigned long		policy_data;
 	unsigned long		directpath_data;
-	float			load;
 	uint64_t		next_poll_tsc;
 
-	/* scheduler data */
-	struct sched_spec	sched_cfg;
-
-	/* the flow steering table */
-	unsigned int		flow_tbl[NCPU];
+	float			load;
 
 	/* runtime threads */
-	unsigned int		thread_count;
-	unsigned int		active_thread_count;
-	struct thread		threads[NCPU];
-	struct thread		*active_threads[NCPU];
 	struct list_head	idle_threads;
-	unsigned int		next_thread_rr; // for spraying join requests/overflow completions
+	struct thread		threads[NCPU];
+
+	/* COLD */
 
 	/* network data */
 	uint32_t		ip_addr;
 
+	struct shm_region	region;
+
 	/* Overfloq queue for completion data */
 	size_t max_overflows;
 	size_t nr_overflows;
 	unsigned long *overflow_queue;
 	struct list_node overflow_link;
 
+	uint16_t		next_thread_rr; // for spraying join requests/overflow completions
+
+	/* scheduler data */
+	struct sched_spec	sched_cfg;
+
+	/* the flow steering table */
+	uint16_t		flow_tbl[NCPU];
+	struct thread		*active_threads[NCPU];
 	int				control_fd;
+	pid_t			pid;
 
 	/* table of physical addresses for shared memory */
 	physaddr_t		page_paddrs[];
@@ -247,7 +255,7 @@ extern struct thread *ts[NCPU];
  */
 static inline void poll_thread(struct thread *th)
 {
-	if (th->ts_idx != UINT_MAX)
+	if (th->ts_idx != UINT16_MAX)
 		return;
 	proc_get(th->p);
 	ts[nrts] = th;
@@ -260,11 +268,11 @@ static inline void poll_thread(struct thread *th)
  */
 static inline void unpoll_thread(struct thread *th)
 {
-	if (th->ts_idx == UINT_MAX)
+	if (th->ts_idx == UINT16_MAX)
 		return;
 	ts[th->ts_idx] = ts[--nrts];
 	ts[th->ts_idx]->ts_idx = th->ts_idx;
-	th->ts_idx = UINT_MAX;
+	th->ts_idx = UINT16_MAX;
 	proc_put(th->p);
 }
 
@@ -305,8 +313,8 @@ struct dataplane {
 	struct rte_mempool	*rx_mbuf_pool;
 	struct shm_region	ingress_mbuf_region;
 
+	uint16_t			nr_clients;
 	struct proc		*clients[IOKERNEL_MAX_PROC];
-	int			nr_clients;
 	struct rte_hash		*ip_to_proc;
 	struct rte_device	*device;
 };
diff --git a/iokernel/directpath/queues.c b/iokernel/directpath/queues.c
index edf67a0..e27ae7c 100644
--- a/iokernel/directpath/queues.c
+++ b/iokernel/directpath/queues.c
@@ -171,7 +171,7 @@ void directpath_handle_completion_eqe_batch(struct mlx5_eqe **eqe, unsigned int
 		return;
 	}
 
-	for (i = 0; i < nr + 4; i++) {
+	for (i = 0; i < nr + 5; i++) {
 
 		/* prefetch cqn to cq entry */
 		if (i < nr)
@@ -201,10 +201,18 @@ void directpath_handle_completion_eqe_batch(struct mlx5_eqe **eqe, unsigned int
 		}
 
 		/* add a core if the proc needs it */
-		if (i > 3) {
+		if (i > 3 && i < nr + 4) {
 			if (!sched_threads_active(ps[i - 4]))
-				sched_add_core(ps[i - 4]);
+				prefetch((void *)ps[i - 4]->policy_data);
 		}
+
+		/* add a core if the proc needs it */
+		if (i > 4) {
+			if (!sched_threads_active(ps[i - 5]))
+				sched_add_core(ps[i - 5]);
+		}
+
+
 	}
 }
 
diff --git a/iokernel/sched.c b/iokernel/sched.c
index d1ed994..e153585 100644
--- a/iokernel/sched.c
+++ b/iokernel/sched.c
@@ -91,7 +91,7 @@ static void sched_steer_flows(struct proc *p)
 
 	/* then assign the rest round-robin */
 	for (i = 0; i < p->thread_count; i++) {
-		if (p->flow_tbl[i] != UINT_MAX)
+		if (p->flow_tbl[i] != UINT16_MAX)
 			continue;
 		th = p->active_threads[j++ % p->active_thread_count];
 		p->flow_tbl[i] = th - p->threads;
@@ -109,14 +109,16 @@ static void sched_enable_kthread(struct thread *th, unsigned int core)
 	th->active = true;
 	th->core = core;
 	list_del_from(&p->idle_threads, &th->idle_link);
-	th->at_idx = p->active_thread_count;
-	p->active_threads[p->active_thread_count++] = th;
-	if (!p->has_directpath)
-		sched_steer_flows(p);
-	if (p->has_vfio_directpath)
+	if (p->has_vfio_directpath) {
+		p->active_thread_count++;
 		directpath_notify_waking(p, th);
-	else
+	} else {
+		th->at_idx = p->active_thread_count;
+		p->active_threads[p->active_thread_count++] = th;
 		poll_thread(th);
+		if (!p->has_directpath)
+			sched_steer_flows(p);
+	}
 
 	if (unlikely(!p->started))
 		p->started = true;
@@ -128,13 +130,17 @@ static void sched_disable_kthread(struct thread *th)
 
 	th->active = false;
 	th->change_tsc = cur_tsc;
-	p->active_threads[th->at_idx] = p->active_threads[--p->active_thread_count];
-	p->active_threads[th->at_idx]->at_idx = th->at_idx;
 	list_add(&p->idle_threads, &th->idle_link);
 	if (!p->has_directpath)
 		sched_steer_flows(p);
-	if (!p->has_vfio_directpath && lrpc_empty(&th->txpktq))
-		unpoll_thread(th);
+	if (!p->has_vfio_directpath) {
+		p->active_threads[th->at_idx] = p->active_threads[--p->active_thread_count];
+		p->active_threads[th->at_idx]->at_idx = th->at_idx;
+		if (lrpc_empty(&th->txpktq))
+			unpoll_thread(th);
+	} else {
+		p->active_thread_count--;
+	}
 }
 
 static struct thread *sched_pick_kthread(struct proc *p, unsigned int core)
@@ -441,7 +447,7 @@ sched_update_kthread_metrics(struct thread *th, bool work_pending)
 }
 
 static void
-sched_measure_kthread_delay(struct thread *th, uint64_t *thread_delay,
+sched_measure_kthread_delay(struct proc *p, struct thread *th, uint64_t *thread_delay,
                             uint64_t *rxq_delay, bool *has_work,
                             bool *standing_queue, uint64_t *next_timer)
 {
@@ -498,20 +504,24 @@ sched_measure_kthread_delay(struct thread *th, uint64_t *thread_delay,
 	 * DIRECTPATH: measure delay and update signals.
 	 * ignore the busy signal here.
 	 */
-	if (th->directpath_hwq.hwq_type == HWQ_MLX5_QSTEER) {
-		bool a, b;
-		sched_measure_hardware_delay(th, &th->directpath_hwq, true, &a, &b, rxq_delay);
-	} else {
+	if (p->has_directpath && !p->has_vfio_directpath) {
+		if (th->directpath_hwq.hwq_type == HWQ_MLX5_QSTEER) {
+			bool a, b;
+			sched_measure_hardware_delay(th, &th->directpath_hwq, true, &a, &b, rxq_delay);
+		} else {
+			tmp = 0;
+			sched_measure_hardware_delay(th, &th->directpath_hwq, true, has_work, standing_queue, &tmp);
+			*thread_delay += tmp;
+		}
+	}
+
+	if (p->has_storage) {
+		/* STORAGE: measure delay and update signals */
 		tmp = 0;
-		sched_measure_hardware_delay(th, &th->directpath_hwq, true, has_work, standing_queue, &tmp);
+		sched_measure_hardware_delay(th, &th->storage_hwq, true, has_work,
+			                         standing_queue, &tmp);
 		*thread_delay += tmp;
 	}
-
-	/* STORAGE: measure delay and update signals */
-	tmp = 0;
-	sched_measure_hardware_delay(th, &th->storage_hwq, true, has_work,
-		                         standing_queue, &tmp);
-	*thread_delay += tmp;
 }
 
 #define EWMA_WEIGHT     0.1f
@@ -542,6 +552,9 @@ static void sched_measure_delay(struct proc *p)
 	uint64_t rxq_delay = 0, consumed_strides, posted_strides, next_poll_tsc;
 	unsigned int i;
 
+	if (!proc_sched_should_poll(p, cur_tsc))
+		return;
+
 	dl.has_work = false;
 	dl.standing_queue = false;
 	dl.parked_thread_busy = false;
@@ -549,9 +562,6 @@ static void sched_measure_delay(struct proc *p)
 	dl.min_delay_us = UINT64_MAX;
 	dl.avg_delay_us = 0;
 
-	if (!proc_sched_should_poll(p, cur_tsc))
-		return;
-
 	next_poll_tsc = UINT64_MAX;
 	consumed_strides = atomic64_read(&p->runtime_info->directpath_strides_consumed);
 
@@ -566,7 +576,7 @@ static void sched_measure_delay(struct proc *p)
 			continue;
 		}
 
-		sched_measure_kthread_delay(th, &delay, &rxq_delay, &busy,
+		sched_measure_kthread_delay(p, th, &delay, &rxq_delay, &busy,
 			                        &dl.standing_queue, &next_timer_tsc);
 
 		if (th->active)
@@ -612,11 +622,6 @@ static void sched_measure_delay(struct proc *p)
 		dl.parked_thread_busy |= sched_threads_active(p) == 0;
 	}
 
-	/* when possible, defer polling this proc until the next timer */
-	if (sched_threads_active(p) == 0 && !dl.has_work &&
-	    directpath_armed && sched_proc_can_unpoll(p))
-	    proc_set_next_poll(p, next_poll_tsc);
-
 	/* don't report parked busy if no threads are active */
 	if (cfg.noidlefastwake && sched_threads_active(p) == 0)
 		dl.parked_thread_busy = false;
@@ -632,6 +637,9 @@ static void sched_measure_delay(struct proc *p)
 	/* notify the scheduler policy of the current delay */
 	if (sched_ops->notify_congested(p, &dl))
 		proc_disable_sched_poll(p);
+	else if (sched_threads_active(p) == 0 && !dl.has_work &&
+	    directpath_armed && sched_proc_can_unpoll(p))
+	    proc_set_next_poll(p, next_poll_tsc);
 }
 
 /*
@@ -697,7 +705,7 @@ rewake:
  */
 void sched_poll(void)
 {
-	static uint64_t last_time = 0;
+	static uint64_t last_time;
 	DEFINE_BITMAP(idle, NCPU);
 	struct core_state *s;
 	uint64_t now;
@@ -710,20 +718,21 @@ void sched_poll(void)
 
 	cur_tsc = rdtsc();
 	now = (cur_tsc - start_tsc) / cycles_per_us;
-	if (now - last_time >= IOKERNEL_POLL_INTERVAL) {
-		int i;
+	if (cur_tsc - last_time >= IOKERNEL_POLL_INTERVAL * cycles_per_us) {
 
 		STAT_INC(SCHED_RUN, 1);
 
 		/* retrieve current network device tick */
 		hw_timestamp_update();
 
-		last_time = now;
+		last_time = cur_tsc;
 		for (i = 0; i < dp.nr_clients; i++) {
+			if (i + 2 < dp.nr_clients)
+				prefetch(dp.clients[i + 2]);
 			p = dp.clients[i];
 			sched_measure_delay(p);
 		}
-	} else if (!cfg.noidlefastwake) {
+	} else if (!cfg.noidlefastwake && !cfg.vfio_directpath) {
 		/* check if any idle directpath runtimes have received I/Os */
 		for (i = 0; i < dp.nr_clients; i++) {
 			p = dp.clients[i];
@@ -822,11 +831,9 @@ int sched_attach_proc(struct proc *p)
 	}
 
 	p->active_thread_count = 0;
-	/* p->active_threads[0] always has most recent thread */
-	p->active_threads[0] = &p->threads[0];
 	list_head_init(&p->idle_threads);
 	for (i = 0; i < p->thread_count; i++) {
-		p->threads[i].core = UINT_MAX;
+		p->threads[i].core = UINT16_MAX;
 		p->threads[i].active = false;
 		list_add(&p->idle_threads, &p->threads[i].idle_link);
 	}
@@ -836,6 +843,7 @@ int sched_attach_proc(struct proc *p)
 		return ret;
 
 	nr_guaranteed += p->sched_cfg.guaranteed_cores;
+	proc_enable_sched_poll(p);
 
 	return 0;
 }
-- 
2.39.2

